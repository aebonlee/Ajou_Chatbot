from typing import Dict, Any, Optional, List
from langgraph.graph import StateGraph, END
from .state import GraphState, GraphStateInfo
from .nodes import node_parse_intent, node_need_more, node_retrieve, node_build_context, node_answer, retrieve_node, \
    generate_node, fallback_node, should_generate
from langchain_core.runnables import RunnableConfig
from .nodes_classify import node_classify
from app.core import config
from langchain_core.prompts import ChatPromptTemplate
from app.services.retriever import get_enhanced_filter, dynamic_retriever
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from app.core.config import LLM_MODEL_NOTICE

def build_graph() -> Any:
    g = StateGraph(GraphState)
    g.add_node("parse_intent", node_parse_intent)
    g.add_node("classify", node_classify)
    g.add_node("need_more", node_need_more)
    g.add_node("retrieve", node_retrieve)
    g.add_node("build_context", node_build_context)
    g.add_node("answer", node_answer)

    g.set_entry_point("parse_intent")
    g.add_edge("parse_intent", "classify")

    def after_classify(s: Dict[str, Any]):
        return "answer" if s.get("skip_rag") else "need_more"
    g.add_conditional_edges("classify", after_classify, {"answer": "answer", "need_more": "need_more"})

    g.add_conditional_edges(
        "need_more",
        lambda s: END if s.get("needs_clarification") else "retrieve",
        {"retrieve": "retrieve", END: END}
    )
    g.add_edge("retrieve", "build_context")
    g.add_edge("build_context", "answer")
    g.add_edge("answer", END)
    return g.compile()

def run_rag_graph(
    *,
    question: str,
    user_id: str = "anonymous",
    persist_dir: str = config.PERSIST_DIR,
    collection: str = config.COLLECTION,
    embedding_model: str = config.EMBEDDING_MODEL,
    topk: int = config.TOPK,
    model_name: str = config.LLM_MODEL,
    temperature: float = config.TEMPERATURE,
    max_tokens: int = config.MAX_TOKENS,
    use_llm: bool = True,
    debug: bool = False,
    scope_colleges: Optional[List[str]] = None,
    scope_depts: Optional[List[str]] = None,
    micro_mode: Optional[str] = None,
    assemble_budget_chars: int = 80000,
    max_ctx_chunks: int = 8,
    rerank: bool = True,
    rerank_model: str = "BAAI/bge-reranker-v2-m3",
    rerank_candidates: int = 40,
) -> Dict[str, Any]:
    graph = build_graph()
    init: GraphState = {
        "question": question,
        "user_id": user_id,
        "context_struct": {},
        "needs_clarification": False,
        "clarification_prompt": None,
        "retrieved": [],
        "context": "",
        "answer": None,
        "llm_answer": None,
        "error": None,
        "category": None,
        "style_guide": None,
        "skip_rag": False,
        "must_include": [],
        "opts": {
            "persist_dir": persist_dir,
            "collection": collection,
            "embedding_model": embedding_model,
            "topk": topk,
            "model_name": model_name,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "use_llm": use_llm,
            "debug": debug,
            "scope_colleges": scope_colleges or [],
            "scope_depts": scope_depts or [],
            "assemble_budget_chars": assemble_budget_chars,
            "max_ctx_chunks": max_ctx_chunks,
            "rerank": rerank,
            "rerank_model": rerank_model,
            "rerank_candidates": rerank_candidates,
        },
    }
    if micro_mode is not None:
        init["opts"]["micro_mode"] = micro_mode

    out: GraphState = graph.invoke(init)  # type: ignore
    hits = out.get("retrieved") or []
    return {
        "question": out.get("question"),
        "answer": out.get("answer"),
        "context": out.get("context"),
        "sources": [h.get("path") or (h.get("metadata") or {}).get("path", "") for h in hits],
        "micro_mode": (init["opts"].get("micro_mode") or "exclude"),
        "error": out.get("error"),
        "clarification_prompt": out.get("clarification_prompt"),
        "llm_answer": out.get("llm_answer"),
    }



#--------------------------------------------
# í•™ì‚¬ê³µí†µ
# -------------------------------------------

def make_graph():
    graph = StateGraph(GraphStateInfo)

    graph.add_node("retrieve", retrieve_node)
    graph.add_node("generate", generate_node)
    graph.add_node("fallback", fallback_node)

    graph.set_entry_point("retrieve")

    graph.add_conditional_edges(
        "retrieve",
        should_generate,
        {
            "generate": "generate",
            "fallback": "fallback",
        },
    )

    graph.add_edge("generate", END)
    graph.add_edge("fallback", END)

    return graph.compile()

_pipeline_cache = {}

def get_cached_pipeline():
    """ê·¸ë˜í”„ íŒŒì´í”„ë¼ì¸ì„ ìºì‹±í•˜ì—¬ ë°˜í™˜"""
    if "graph" in _pipeline_cache:
        return _pipeline_cache["graph"]
    app = make_graph()
    _pipeline_cache["graph"] = app
    return app


def route_query_sync(question: str, departments: List[str] = None, selected_list: List[str] = None):
    """
    ê·¸ë˜í”„ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜.
    departments ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ë©”íƒ€ë°ì´í„° í•„í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    if departments is None:
        departments = []
    app = get_cached_pipeline()

    inputs = {"question": question, "departments": departments, "user_selected_list": selected_list}

    final_state = app.invoke(inputs)

    return {
        "answer": final_state.get("answer", "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."),
        "documents": final_state.get("documents", [])
    }





################ê³µì§€ì‚¬í•­#####################
    
# -------------------------------
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
# -------------------------------
template = """
ë‹¹ì‹ ì€ ì•„ì£¼ëŒ€í•™êµì˜ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ê³µì§€ì‚¬í•­ ì•ˆë‚´ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì•„ë˜ì— ì œê³µëœ "ë¬¸ì„œ" ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ "ì§ˆë¬¸"ì— ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

**ë‹µë³€ í˜•ì‹:**
- "ë„¤, [ì§ˆë¬¸ë‚´ìš©]ì— ëŒ€í•œ ê³µì§€ì‚¬í•­ì„ ì°¾ì•„ë“œë¦´ê²Œìš”!" ë¡œ ì¹œê·¼í•˜ê²Œ ì‹œì‘
- ì°¾ì€ ê³µì§€ì‚¬í•­ë“¤ì„ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì„œ ì œê³µ:

ğŸ“Œ **[ì œëª©]**
ğŸ”— **ë§í¬**: [URL]
ğŸ“ **ìš”ì•½**: [ì£¼ìš” ë‚´ìš© 1-2ì¤„]

**íŠ¹ë³„ ì§€ì‹œ:**
1. ì œê³µëœ ë¬¸ì„œì— ê´€ë ¨ ê³µì§€ì‚¬í•­ì´ ìˆìœ¼ë©´ ìœ„ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
2. ì—¬ëŸ¬ ê³µì§€ê°€ ìˆìœ¼ë©´ ëª¨ë‘ ğŸ“Œ ì•„ì´ì½˜ê³¼ í•¨ê»˜ ë‚˜ì—´í•´ì£¼ì„¸ìš”.
3. ê´€ë ¨ ë¬¸ì„œê°€ ì „í˜€ ì—†ë‹¤ë©´: "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ í•´ë‹¹ ë‚´ìš©ì˜ ê³µì§€ì‚¬í•­ì„ ì°¾ì„ ìˆ˜ ì—†ë„¤ìš”. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì‹œê±°ë‚˜, í•™ê³¼ ì‚¬ë¬´ì‹¤ì— ì§ì ‘ ë¬¸ì˜í•´ë³´ì‹œëŠ” ê±´ ì–´ë–¨ê¹Œìš”?"
4. ë§ˆì§€ë§‰ì— "ë” ìì„¸í•œ ë‚´ìš©ì€ ìœ„ ë§í¬ë¥¼ í†µí•´ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤!" ë¡œ ë§ˆë¬´ë¦¬
5. ì œëª©, ë§í¬, ìš”ì•½ì€ ì¤„ë°”ê¿ˆ í›„ ì¶œë ¥í•´ì£¼ì„¸ìš”

---
ë¬¸ì„œ:
{context}

---
ì§ˆë¬¸: {question}

ë‹µë³€:
"""
prompt = ChatPromptTemplate.from_template(template)


def format_docs(docs):
    lines = []
    for d in docs or []:
        md = getattr(d, "metadata", {}) or {}
        title = md.get("title") or ""
        url = md.get("url") or ""

        # í•µì‹¬: ì‹¤ì œ ë¬¸ì„œ ë‚´ìš©ë„ í¬í•¨
        content = getattr(d, 'page_content', '') or ''

        lines.append(f"- ì œëª©: {title}")
        lines.append(f"- URL: {url}")
        if content:
            lines.append(f"- ë‚´ìš©: {content}")
        lines.append("")  # ë¹ˆ ì¤„ë¡œ êµ¬ë¶„

    return "\n".join(lines) if lines else "(ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)"

# ì…ë ¥ ì „ì²˜ë¦¬ (question + filter ë™ì‹œ ìƒì„±)
# -------------------------------
def enrich_inputs(x):
    f = get_enhanced_filter(x["question"])
    print("[NOTICE] filter =", f)
    return {"question": x["question"], "filter": f}

def _ctx_builder(d):
    docs = dynamic_retriever(d["question"], d["filter"])
    print(f"[NOTICE] retrieved {len(docs)} docs")  # ğŸ” ê°œìˆ˜ í™•ì¸
    ctx = format_docs(docs)
    print("[NOTICE] context:\n", ctx)              # ğŸ” LLMì— ì£¼ëŠ” ë¬¸ìì—´
    return {"context": ctx, "question": d["question"]}

#  RAG ì²´ì¸ êµ¬ì¶•
llm = ChatGoogleGenerativeAI(model=LLM_MODEL_NOTICE)


rag_chain = (
    RunnableLambda(enrich_inputs)
    | {
        "question": RunnableLambda(lambda d: d["question"]),
        "raw_docs": RunnableLambda(lambda d: dynamic_retriever(d["question"], d["filter"])),
      }
    | RunnableLambda(lambda d: {"question": d["question"], "context": format_docs(d["raw_docs"])})
    | prompt
    | llm
    | StrOutputParser()
)