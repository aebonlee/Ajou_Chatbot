"""
í´ë” íŠ¸ë¦¬ ì¸ë±ì„œ:
- ì…ë ¥: data/<college>/<dept>/*.md
- ì²˜ë¦¬: H1/H2/H4 ë‹¨ìœ„ë¡œ ì²­í¬ ìª¼ê°œê¸°, ë©”íƒ€í•„ë“œ ì €ì¥(í´ë¦° í…ìŠ¤íŠ¸ë§Œ ì €ì¥)
- ê¸´ ì„¹ì…˜ì€ 1200ì ìœˆë„ìš°/200ì ì˜¤ë²„ë©ìœ¼ë¡œ ë¶„í• 
- (í•µì‹¬) source_path + line_s/e + sec_seq/leaf_seq/chunk_idx + order_key ì €ì¥
  â†’ ì„¹ì…˜ì„ "íŒŒì¼ì—ì„œ" ë³µì›í•˜ê³ , ì›ë¬¸ ìˆœì„œëŒ€ë¡œ ìŠ¤í‹°ì¹­ ê°€ëŠ¥
"""
import os, uuid, glob, re
from typing import List, Dict, Optional, Tuple, Any
from .storage import get_client, get_collection, add
from .textutil import HDR_RE, looks_like_term_header, parse_year_semester
from bs4 import BeautifulSoup, Tag
from langchain_core.documents import Document
from app.core.config import PDF_FILES

# ------------------ ìœ í‹¸ ------------------
def _read(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()

def _slice(lines: List[str], s: int, e: int) -> str:
    block = lines[s:e]
    while block and block[0].strip() == "":
        block.pop(0)
    return "\n".join(block).strip()

def _find_headers(lines: List[str]) -> List[Tuple[int, int, str]]:
    headers = []
    for i, ln in enumerate(lines):
        m = HDR_RE.match(ln)
        if m:
            headers.append((i, len(m.group(1)), m.group(2).strip()))
    return headers

def _make_path(college: str, dept: str, major: Optional[str], sec=None, leaf=None) -> str:
    parts = [college, dept]
    if major: parts.append(major)
    if sec: parts.append(sec)
    if leaf: parts.append(leaf)
    return " > ".join([p for p in parts if p])

def _dedup_adjacent(text: str) -> str:
    out, prev = [], None
    for ln in (text or "").splitlines():
        cur = ln.strip()
        if cur and cur == prev:
            continue
        out.append(ln)
        prev = cur
    return "\n".join(out)

def _compact(t: str) -> str:
    return re.sub(r"\n{3,}", "\n\n", t or "").strip()

def _catalog_year_from_name(fname: str) -> Optional[str]:
    m = re.match(r"(\d{4})[_-]", fname or "")
    return f"{m.group(1)}í•™ë…„ë„" if m else None

def _split_long_text(text: str, max_len: int = 1200, overlap: int = 200) -> List[str]:
    """
    ë¬¸ë‹¨ ìš°ì„  ëˆ„ì  â†’ ì´ˆê³¼ ì‹œ ê¸€ì ìœˆë„ìš°(ì˜¤ë²„ë© í¬í•¨)
    """
    text = _compact(_dedup_adjacent(text or ""))
    if len(text) <= max_len:
        return [text]
    paras = [p.strip() for p in re.split(r"\n\s*\n", text) if p.strip()]
    chunks: List[str] = []
    cur = ""
    for p in paras:
        if len(cur) + len(p) + 2 <= max_len:
            cur = (cur + "\n\n" + p).strip() if cur else p
        else:
            if cur:
                chunks.append(cur)
            cur = p
    if cur:
        chunks.append(cur)

    final: List[str] = []
    for c in chunks:
        if len(c) <= max_len:
            final.append(c)
        else:
            i = 0
            while i < len(c):
                final.append(c[i:i + max_len])
                if i + max_len >= len(c):
                    break
                i = max(0, i + max_len - overlap)
    return final

def _emit_chunk(
    chunks: List[Dict],
    *,
    base_id: str,
    path: str,
    meta: Dict,
    body: str,
    sec_seq: int,
    leaf_seq: int,
) -> None:
    """
    - ì €ì¥ ë¬¸ì„œëŠ” 'í´ë¦° í…ìŠ¤íŠ¸'ë§Œ (ë¬¸ì„œ ì²« ì¤„ì— [PATH] ì ˆëŒ€ ë„£ì§€ ì•ŠìŒ)
    - ìˆœì„œ ë³µì›ì„ ìœ„í•´ chunk_idxì™€ order_keyë¥¼ ë©”íƒ€ì— ë„£ìŒ
    """
    parts = _split_long_text(body, max_len=1200, overlap=200)
    for idx, part in enumerate(parts):
        order_key = f"{sec_seq:03d}.{leaf_seq:03d}.{idx:04d}"
        meta2 = {
            **meta,
            "path": path,
            "chunk_idx": idx,
            "sec_seq": sec_seq,
            "leaf_seq": leaf_seq,
            "order_key": order_key,
        }
        chunks.append({
            "id": f"{base_id}_{idx}" if len(parts) > 1 else base_id,
            "text": (part or "").strip(),   # ğŸŸ¢ í´ë¦° í…ìŠ¤íŠ¸
            "metadata": meta2,
        })

# ------------------ ì²­í‚¹ ------------------
def _chunk_md(md_text: str, college: str, dept: str, source_file: str, catalog_year: Optional[str]) -> List[Dict]:
    lines = (md_text or "").splitlines()
    headers = _find_headers(lines)
    h1_idxs = [i for i, lvl, _ in headers if lvl == 1]
    bounds = [(h1, (h1_idxs[idx+1] if idx+1 < len(h1_idxs) else len(lines))) for idx, h1 in enumerate(h1_idxs)]

    chunks: List[Dict] = []
    for s, e in bounds:
        m = HDR_RE.match(lines[s])
        h1_title = m.group(2).strip() if m else ""
        major = None if h1_title.endswith("í•™ê³¼") else h1_title
        is_micro = "ë§ˆì´í¬ë¡œì „ê³µ" in (major or "")
        program_type = "micro" if is_micro else "major"

        locals_ = [(i, lvl, t) for i, lvl, t in headers if s <= i < e]
        h2s = [(i, t) for i, lvl, t in locals_ if lvl == 2]

        # ì„¹ì…˜ ì‹œí€€ìŠ¤ ì¹´ìš´í„°
        sec_seq = 0

        if not h2s:
            path = _make_path(college, dept, major)
            sec_id = f"doc_{uuid.uuid4().hex}"
            meta = {
                "college": college, "dept": dept, "major": major or "í•™ê³¼ê³µí†µ",
                "section": None, "year": None, "semester": None,
                "catalog_year": catalog_year, "chunk_type": "doc",
                "source": os.path.basename(source_file), "source_path": source_file,
                "parent_id": None, "section_id": sec_id,
                "line_s": s+1, "line_e": e,
                "program_type": program_type, "is_micro": "Y" if is_micro else "N",
            }
            body = _compact(_slice(lines, s+1, e))
            _emit_chunk(
                chunks, base_id=sec_id, path=path, meta=meta, body=body,
                sec_seq=sec_seq, leaf_seq=0
            )
            continue

        for idx, (h2_line, h2_title) in enumerate(h2s):
            sec_seq += 1
            h2_start, h2_end = h2_line, (h2s[idx+1][0] if idx+1 < len(h2s) else e)
            path = _make_path(college, dept, major, h2_title)
            sec_id = f"sec_{uuid.uuid4().hex}"
            meta_sec = {
                "college": college, "dept": dept, "major": major or "í•™ê³¼ê³µí†µ",
                "section": h2_title, "year": None, "semester": None,
                "catalog_year": catalog_year, "chunk_type": "sec",
                "source": os.path.basename(source_file), "source_path": source_file,
                "parent_id": None, "section_id": sec_id,
                "line_s": h2_start+1, "line_e": h2_end,
                "program_type": program_type, "is_micro": "Y" if is_micro else "N",
            }
            body_sec = _compact(_slice(lines, h2_start+1, h2_end))
            # leaf_seq=0 ìœ¼ë¡œ ì„¹ì…˜ ë³¸ë¬¸ emit
            _emit_chunk(
                chunks, base_id=sec_id, path=path, meta=meta_sec, body=body_sec,
                sec_seq=sec_seq, leaf_seq=0
            )

            # í•˜ìœ„ í•™ë…„/í•™ê¸°(H4)
            inner = [(i, lvl, t) for i, lvl, t in locals_ if h2_start < i < h2_end]
            h4s = [(i, t) for i, lvl, t in inner if lvl == 4]

            leaf_seq = 0
            for k, (h4_line, h4_title) in enumerate(h4s):
                if not looks_like_term_header(h4_title):
                    continue
                leaf_seq += 1
                leaf_s, leaf_e = h4_line, (h4s[k+1][0] if k+1 < len(h4s) else h2_end)
                year, sem = parse_year_semester(h4_title)
                path_leaf = _make_path(college, dept, major, h2_title, h4_title)
                term_id = f"term_{uuid.uuid4().hex}"
                meta_leaf = {
                    "college": college, "dept": dept, "major": major or "í•™ê³¼ê³µí†µ",
                    "section": h2_title, "year": year, "semester": sem,
                    "catalog_year": catalog_year, "chunk_type": "term",
                    "source": os.path.basename(source_file), "source_path": source_file,
                    "parent_id": sec_id, "section_id": term_id,
                    "line_s": leaf_s+1, "line_e": leaf_e,
                    "program_type": program_type, "is_micro": "Y" if is_micro else "N",
                }
                body_leaf = _compact(_slice(lines, leaf_s+1, leaf_e))
                _emit_chunk(
                    chunks, base_id=term_id, path=path_leaf, meta=meta_leaf, body=body_leaf,
                    sec_seq=sec_seq, leaf_seq=leaf_seq
                )
    return chunks

# ------------------ ì—”íŠ¸ë¦¬ ------------------
def index_tree(root: str, persist_dir: str, collection: str, embedding_model: str) -> int:
    root = os.path.abspath(root)
    if not os.path.isdir(root):
        raise FileNotFoundError(root)
    client = get_client(persist_dir)
    col = get_collection(client, collection, embedding_model)

    total = 0
    for college in sorted(d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))):
        for dept in sorted(d for d in os.listdir(os.path.join(root, college)) if os.path.isdir(os.path.join(root, college, d))):
            md_glob = os.path.join(root, college, dept, "*.md")
            for md_path in glob.glob(md_glob):
                md = _read(md_path)
                cy = _catalog_year_from_name(os.path.basename(md_path))
                chunks = _chunk_md(md, college, dept, md_path, cy)
                if not chunks:
                    continue
                add(
                    col,
                    [c["id"] for c in chunks],
                    [c["text"] for c in chunks],
                    [c["metadata"] for c in chunks],
                )
                total += len(chunks)
                print(f"[INDEX] {college}/{dept}/{os.path.basename(md_path)} -> {len(chunks)} chunks")
    print(f"Done. Total chunks: {total}")
    return total


# --------------------------------------------
# í•™ì‚¬ê³µí†µ
# -------------------------------------------

def chunk_markdown_file(file_path: str, source_metadata: Dict[str, Any] = None) -> List[Document]:
    """
    Markdown/HTML íŒŒì¼ì„ í˜ì´ì§€ë³„ë¡œ ì²­í‚¹í•˜ê³ , ##ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ë˜,
    í˜ì´ì§€ ê²½ê³„ì—ì„œ í—¤ë” ê³„ì¸µì„ ì˜¬ë°”ë¥´ê²Œ ì „íŒŒí•˜ì—¬ LangChain Document ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if source_metadata is None:
        source_metadata = {}

    EXCLUDED_H1_HEADERS = {"í•™ì¹™", "í•™ì‚¬ë ¥", "ëŒ€í•™ìƒí™œì•ˆë‚´", "ëŒ€í•™ìƒí™œ", "ì´ëŒ"}

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            full_content = f.read()
    except FileNotFoundError:
        print(f"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {file_path}")
        return []

    pages = full_content.split('---')
    all_chunks = []

    header_context = {"h1": "", "h2": "", "h3": "", "h4": ""}

    for i, page_content in enumerate(pages):
        page_number = i + 1
        if not page_content.strip():
            continue

        # 1. ì •ì œ
        cleaned_content = re.sub(r'2025 ì•„ì£¼ëŒ€í•™êµ ìš”ëŒ|AJOU UNIVERSITY', '', page_content)
        cleaned_content = re.sub(r'ã€?\[?\s*6\..*?\]?ã€‘?', '', cleaned_content)
        cleaned_content = re.sub(r'(?:ì•„ì£¼ëŒ€í•™êµ\s*\d+|\d+\s*AJOU UNIV\.?|AU \d+ .*)', '', cleaned_content)
        cleaned_content = re.sub(r'^\s*AU\s*$', '', cleaned_content, flags=re.MULTILINE)

        # 2. í…Œì´ë¸” ë³€í™˜
        soup = BeautifulSoup(f"<div>{cleaned_content}</div>", 'lxml')
        for table in soup.find_all('table'):
            table_text = _convert_table_to_text(table)
            table.replace_with(table_text)
        full_text_on_page = soup.get_text(separator='\n')

        # 3. ë‚´ìš© ì—†ëŠ” í—¤ë” ìµœì¢… ì‚­ì œ
        full_text_on_page = re.sub(r'^#+\s*$', '', full_text_on_page, flags=re.MULTILINE).strip()
        if not full_text_on_page:
            continue

        # 4. ## ê¸°ì¤€ìœ¼ë¡œ ì²­í¬ ë¶„í• 
        text_blocks = re.split(r'(?=\n##\s)', full_text_on_page)

        for block_idx, block in enumerate(text_blocks):
            block = block.strip()
            if not block or len(block.split()) < 3:
                continue

            if "í•™ ìœ„ ê¸°" in block:
                continue

            final_content = block

            # 5. ê³ ì•„ ì²­í¬ ì²˜ë¦¬
            if block_idx == 0 and not block.startswith('##'):
                prefixes = []
                path_parts = [v for k, v in sorted(header_context.items()) if v]
                if path_parts:
                    prefixes.append(f"[ì´ì „ ë¬¸ì„œ ê²½ë¡œ: {' > '.join(path_parts)}]")
                if prefixes:
                    final_content = "\n".join(prefixes) + f"\n\n{block}"

            # 6. í‘œì¤€ ì²­í¬ ì²˜ë¦¬
            elif header_context.get("h1"):
                final_content = f"[ëŒ€ì£¼ì œ: {header_context['h1']}]\n\n{block}"

            metadata = {
                **source_metadata,
                'page': page_number,
                'section_title': block.split('\n')[0].strip().replace('## ', '')
            }
            all_chunks.append(Document(page_content=final_content, metadata=metadata))

        # 7. ë‹¤ìŒ í˜ì´ì§€ë¥¼ ìœ„í•´ í˜„ì¬ í˜ì´ì§€ì˜ ìµœì¢… í—¤ë” ê³„ì¸µ ì—…ë°ì´íŠ¸
        headers_on_page = re.findall(r'(#+)\s([^\n]+)', cleaned_content)
        for hashes, text in headers_on_page:
            level = len(hashes)
            header_key = f'h{level}'
            if header_key in header_context:
                if level == 1:
                    if text.strip() in EXCLUDED_H1_HEADERS:
                        header_context = {k: "" for k in header_context}
                    else:
                        header_context['h1'] = text.strip()
                        for l in range(2, 5): header_context[f'h{l}'] = ""
                else:
                    header_context[header_key] = text.strip()
                    for l in range(level + 1, 5): header_context[f'h{l}'] = ""

    return all_chunks


def _convert_table_to_text(table_tag: Tag) -> str:
    """
    ëª¨ë“  ì¢…ë¥˜ì˜ HTML í…Œì´ë¸”ì„ ì•ˆì •ì ìœ¼ë¡œ ìì—°ì–´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - rowspanì— ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ” ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸í™”
    """
    thead = table_tag.find('thead')
    header_rows = thead.find_all('tr') if thead else []

    # theadê°€ ì—†ë‹¤ë©´ í…Œì´ë¸”ì˜ ì²«ë²ˆì§¸ trì„ í—¤ë”ë¡œ ê°„ì£¼í•  ìˆ˜ ìˆìŒ
    if not header_rows and not thead:
        first_row = table_tag.find('tr')
        if first_row and first_row.find('th'):
            header_rows = [first_row]

    header_matrix = [[None] * 50 for _ in range(len(header_rows))]
    for r_idx, row in enumerate(header_rows):
        cells = row.find_all(['th', 'td'])
        c_idx = 0
        for cell in cells:
            while header_matrix[r_idx][c_idx] is not None: c_idx += 1
            rowspan, colspan = int(cell.get('rowspan', 1)), int(cell.get('colspan', 1))
            text = ' '.join(cell.get_text(strip=True).split())
            for i in range(rowspan):
                for j in range(colspan):
                    if r_idx + i < len(header_rows) and c_idx + j < 50:
                        header_matrix[r_idx + i][c_idx + j] = text
            c_idx += colspan

    if not header_matrix:
        tr = table_tag.find('tr')
        num_cols = len(tr.find_all(['th', 'td'])) if tr else 0
        final_headers = [f"column_{i + 1}" for i in range(num_cols)]
    else:
        num_cols = 0
        for r in header_matrix:
            num_cols = max(num_cols, len(r))
        final_headers = [''] * num_cols
        for c in range(num_cols):
            header_parts = [header_matrix[r][c] for r in range(len(header_matrix)) if
                            c < len(header_matrix[r]) and header_matrix[r][c]]
            final_headers[c] = '_'.join(dict.fromkeys(header_parts))

    # tbodyê°€ ì—†ì–´ë„ ëª¨ë“  trì„ ëŒ€ìƒìœ¼ë¡œ í•˜ë˜, í—¤ë”ë¡œ ì‚¬ìš©ëœ í–‰ì€ ì œì™¸
    all_rows = table_tag.find_all('tr')
    body_rows = [row for row in all_rows if row not in header_rows]

    if not body_rows: return ""

    result_texts = ["\n[í‘œ ì‹œì‘]"]
    row_context = {}  # rowspan ë°ì´í„°ë¥¼ ê¸°ì–µí•˜ê¸° ìœ„í•œ ë³€ìˆ˜

    for row in body_rows:
        cells = row.find_all('td')
        if not cells: continue

        row_data = [(' '.join(cell.get_text(strip=True).split()), int(cell.get('rowspan', 1))) for cell in cells]

        current_row_pairs = []
        cell_idx = 0
        col_idx = 0

        # rowspanìœ¼ë¡œ ì¸í•´ ëˆ„ë½ëœ ì…€ ë³µì› ë° ë°ì´í„°-í—¤ë” ë§¤ì¹­
        while col_idx < len(final_headers):
            if col_idx in row_context and row_context[col_idx]['remaining'] > 0:
                # ì´ì „ í–‰ì—ì„œ ë³‘í•©ëœ ì…€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
                current_row_pairs.append(f"{final_headers[col_idx]}: {row_context[col_idx]['text']}")
                row_context[col_idx]['remaining'] -= 1
                col_idx += 1
            elif cell_idx < len(row_data):
                text, rowspan = row_data[cell_idx]
                if rowspan > 1:
                    row_context[col_idx] = {'text': text, 'remaining': rowspan - 1}
                if text:
                    current_row_pairs.append(f"{final_headers[col_idx]}: {text}")
                cell_idx += 1
                col_idx += 1
            else:
                break

        if current_row_pairs:
            result_texts.append(", ".join(current_row_pairs))

    result_texts.append("[í‘œ ë]\n")
    return "\n".join(result_texts)


def process_documents(pdf_paths: List[str]) -> List[Document]:
    """ì£¼ì–´ì§„ ê²½ë¡œì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ì½ê³  ì²­í‚¹í•˜ì—¬ Document ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""

    pdf_title_map = {
        "2025_rules.md": "2025ë…„ë„_í•™ì¹™_í•™ìƒì¤€ì¹™_ì¥í•™ê¸ˆ_ìƒí™œê´€_ê³µí•™ì¸ì¦",
        "2025_overview.md": "2025ë…„ë„_í•™ì‚¬ë ¥_ì´ëŒ_ê¸°êµ¬í‘œ_êµ¬ì„±_í•™êµë²•ì¸_ëŒ€ìš°í•™ì›",
        "2025_campus_life.md": "2025ë…„ë„_ëŒ€í•™ìƒí™œ_í•™ì‚¬_ì¥í•™_í•™ìƒêµë¥˜_ë³µì§€_ë¬¸í™”ì‹œì„¤_ê³ ì‹œì¤€ë¹„_í•™ìƒë³‘ì‚¬_ì˜ˆë¹„êµ°_ë¯¼ë°©ìœ„êµìœ¡",
    }

    all_chunks = []
    for path in pdf_paths:
        file_name = os.path.basename(path)
        source_id = next((ctype.value for ctype, fpath in PDF_FILES.items() if fpath == path), "unknown")

        source_metadata = {
            "source": source_id,
            "title": pdf_title_map.get(file_name, file_name)  # title ë§µì—ì„œ ì œëª© ì¡°íšŒ
        }
        chunks = chunk_markdown_file(path, source_metadata)
        all_chunks.extend(chunks)
    return all_chunks